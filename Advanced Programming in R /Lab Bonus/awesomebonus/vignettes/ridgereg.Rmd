---
title: "awesomebonus: A ridge regression package"
author: "Martin Svensson and Laura Julià Melis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{awesomebonus: A ridge regression package}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

## 1. Introduction.
The **awesomebonus** package contains an object with class ridgereg, created with the RC object oriented system. This object is helpful to handle ridge regression models as it provides multiple methods such as `pred()` and `coef()` to obtain the vector of fitted values and the regressions coefficients of a given regression model, respectively.

## 2. ridgereg( ) description.
The class object `ridgereg()` stores the statistics seen in the section above as *fields* and contains multiple functions (*methods*) to obtain outputs related to ridge regression models. The name and a brief description of each one is listed below. 

**1. Fields.**
 
  - `formula:` an object of class "formula".
  - `data:` a data frame.
  - `p_values:` the p-values for each coefficient.
  - `lambda:` an integer of ridge constant.
  - `fitted_values:` the fitted values.

**2. Methods.**
 
  - `initialize()` calculates all the statistics described in section **2** from formula and data.
  - `print()` prints out the coefficients and coefficient names.
  - `pred()` returns the predicted values.
  - `coef()` returns the coefficients as a named vector.

## 3. Example.
This section shows how to use `ridgereg()` on a well known dataset such as the BostonHousing dataset (it can be found in the mlbench package).
```{r include=FALSE}
library(awesomebonus)
library(caret)
library(mlbench)
```

### 3.1. Initialization.
First we will divide the dataset into a test and training dataset:
```{r}
data("BostonHousing")

dataSet <- dplyr::select(BostonHousing, -c(chas))

inTrain <- createDataPartition(
  y = dataSet$medv,
  ## the outcome data are needed
  p = .8,
  ## The percentage of data in the
  ## training set
  list = FALSE
)

training <- dataSet[ inTrain,] 
testing  <- dataSet[-inTrain,]
```

### 3.2. Fitting a linear regression model.
First we sill fit a regular linear regression model using lm from build in caret package.
```{r}
linregFit <- train(medv ~ .,
                   data= training,
                   method = 'lm', # to fit linear regression with forward selection
                   preProc = c("center", "scale") # center and scale the predictors
                   )
linregFit
```

Now, we will fit a linear regression model with forward selection of covariates on the *training* dataset.
```{r}
library(leaps)
linforFit <- train(medv ~ .,
                   data= training,
                   method = 'leapForward', # to fit linear regression with forward selection
                   preProc = c("center", "scale") # center and scale the predictors
                   )
linforFit
```


### 3.3. Evaluating the performance of this model on the training dataset.
The RMSE indicates how close the observed data points are to the model’s predicted values, it can be interpreted as the standard deviation of the unexplained variance, and has the same units as the response variable. A low RMSE indicates a good fit and if we observe that our dependent variable has a range of 44.4 (values between 5.6 and 50) and a standard deviation of 8.74 then we can conclude that the RMSE is low and it indicates a good fit.

The Rsquared is a goodness-of-fit measure ans also indicates how close the data are to the fitted regression line. The closer to 1 the $R^2$ value is, the better the model fits the data because it means that the model explains a lot of the variability of the response data around its mean. In our case, the model explains 68.8% of the variability so, again, we can say that this is a good model.


### 3.4. Fitting a ridge regression model using your ridgereg() function to the training dataset.

To use our own method, the ridgereg() function, we have created the `ridgereg_model()` function in order to generate a custom model object for the caret package (for more information and references about using one's own model in train, run `?ridgereg_model`)

```{r}
ridge_model <- ridgereg_model()

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 5)

rGrid <- expand.grid(lambda = 1*10**(c(1:10)))

set.seed(825)
rrTune <- train(form = medv ~ .,
                data = training,
                method = ridge_model,
                trControl = fitControl, 
                tuneGrid = rGrid)


```


### 3.5. Finding the best hyperparameter value for λ 
```{r echo = FALSE}
library(knitr)
kable(rrTune$results, caption = "Evaluation of the Hyper parameters")

```

Looking at RMSE vs. lambda in a plot.

```{r echo = FALSE, fig.align='center'}
library(ggplot2)
ggplot(rrTune$results) + 
  geom_point(aes(x= log10(lambda), y = RMSE, colour="#2c3e50", size=2)) + 
  geom_line(aes(x= log10(lambda), y = RMSE, colour="#2980b9", size=1)) + 
  theme(legend.position = "none") + 
  scale_x_continuous(breaks=seq(0,24,2))
```

We can conclude that lambda $1*10^5$ or $1*10^6$ gives the best RMSE and with the lowest lambda. Using $lambda = 1*10^6$ for the test dataset.

### 3.6. Evaluate performance of all three models on test dataset. 
As our outcome is numeric, we will use the function `postResample` in order to estimate the root mean squared error (RMSE), the coefficient of determination ($R^2$), and the mean absolute error (MAE).
```{r}
pred <- predict(linregFit, testing)
postResample(pred = pred, obs = testing$medv)
```

And the same for the forward looking model.
```{r}
forpred <- predict(linforFit, testing)
postResample(pred = forpred, obs = testing$medv)
```

```{r}
my_ridge <- ridgereg$new(medv~., data=training, lambda = 10**6)
X_test <- dplyr::select(testing, -medv)
ridge_pred <- my_ridge$predict(X_test)
postResample(pred = ridge_pred, obs = testing$medv)
```

We can see that in this example both the linear models have better performance for all three evaluation variables (RMSE, $R^2$ and MAE). Conclusion is that this dataset was well-adapted for the linear models. 


## References.
* Ridge Regression: <https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf>
* Caret package: <https://cran.r-project.org/web/packages/caret/caret.pdf>
* Vignettes: <http://r-pkgs.had.co.nz/vignettes.html>